{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os, sys, pdb, pickle\n",
    "from multiprocessing import Process, Pool\n",
    "import time\n",
    "# from sets import Set\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22089, 32, 96) (3069, 32, 96) (3031, 32, 96) (22089,) (3069,) (3031,)\n"
     ]
    }
   ],
   "source": [
    "data_folder = '../outfiles/'\n",
    "data_version = 'processed-12_mel_noiseless'\n",
    "\n",
    "with open(data_folder + data_version + '_ys.pkl', 'rb') as f:\n",
    "    (ytr, yva, yte, classes) = pickle.load(f)\n",
    "    num_classes = len(classes)\n",
    "with open(data_folder + data_version + '_Xte.npy', 'rb') as f:\n",
    "    Xte = np.load(f)\n",
    "with open(data_folder + data_version + '_Xva.npy', 'rb') as f:\n",
    "    Xva = np.load(f)\n",
    "with open(data_folder + data_version + '_Xtr.npy', 'rb') as f:\n",
    "    Xtr = np.load(f)\n",
    "print(Xtr.shape,Xva.shape,Xte.shape,ytr.shape,yva.shape,yte.shape)\n",
    "\n",
    "mu = np.mean(Xtr, 0)\n",
    "std = np.std(Xtr, 0)\n",
    "Xtr = ((Xtr - mu) / std)\n",
    "Xva = ((Xva - mu) / std)\n",
    "Xte = ((Xte - mu) / std)\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "Ytr = to_categorical(ytr, num_classes)\n",
    "Yva = to_categorical(yva, num_classes)\n",
    "Yte = to_categorical(yte, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pte):\n",
    "    conf_te = confusion_matrix(yte, pte)\n",
    "\n",
    "    # https://matplotlib.org/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "    fig, (ax1, ax2, axc) = plt.subplots(1,3,figsize=(18, 6))\n",
    "\n",
    "    ax1.plot(range(epochs), hist['TrLoss'], 'ro-', label='Training')\n",
    "    ax1.plot(range(epochs), hist['VaLoss'], 'bo-', label='Validation')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss')\n",
    "\n",
    "    ax2.plot(range(epochs), hist['TrAcc'], 'ro-', label='Training')\n",
    "    ax2.plot(range(epochs), hist['VaAcc'], 'bo-', label='Validation')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Training Accuracy')\n",
    "\n",
    "    im = axc.imshow(np.log(1 + conf_te), cmap='jet')\n",
    "    axc.set_xticks(range(num_classes))\n",
    "    axc.set_yticks(range(num_classes))\n",
    "    axc.set_xticklabels(classes)\n",
    "    axc.set_yticklabels(classes)\n",
    "    plt.setp(axc.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            text = axc.text(j,i,conf_te[i,j], ha='center', va='center', color='w')\n",
    "    axc.set_title('Confusion Matrix')\n",
    "    axc.grid(False)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(x, dim):\n",
    "    xsize = x.size()\n",
    "    dim = x.dim() + dim if dim < 0 else dim\n",
    "    x = x.contiguous()\n",
    "    x = x.view(-1, *xsize[dim:])\n",
    "    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\n",
    "    return x.view(xsize)\n",
    "\n",
    "class Quantize(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Quantize, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class StochasticDropout(nn.Module):\n",
    "    def __init__(self, sigma):\n",
    "        super(StochasticDropout, self).__init__()\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training: sdo = torch.from_numpy(np.exp(np.random.normal(loc=-self.sigma**2/2, scale=self.sigma, size=list(x.size()))).astype('f')).cuda()\n",
    "        else: sdo = 1\n",
    "        return x * sdo\n",
    "\n",
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, momentum=0.05, maxpool=False):\n",
    "        super(ResLayer, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.maxpool = maxpool\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(1,1), padding=(0,0), bias=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3,3), padding=(1,1), bias=True)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=(3,3), padding=(1,1), bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels, momentum=momentum)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        xi = x\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn1(x + xi)\n",
    "        x = F.relu(x)\n",
    "        if self.maxpool: x = F.max_pool2d(x, 2, 2)\n",
    "        return x\n",
    "\n",
    "class HardSigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HardSigmoid, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (torch.clamp(x, min=-2.5, max=2.5) + 2.5) / 5.0\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "class liGRU(nn.Module):\n",
    "    def __init__(self, units, inp_dim, use_cuda=True):\n",
    "        super(liGRU, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.input_dim = inp_dim\n",
    "        self.ligru_lay = [units]\n",
    "        self.drop = {\n",
    "            'in': ('binomial', 0.2),\n",
    "            'orig': ('binomial', 0.2),\n",
    "            'recur': ('binomial', 0.2),\n",
    "        }\n",
    "        self.ligru_use_batchnorm_inp = False\n",
    "        self.ligru_use_laynorm_inp = False\n",
    "        self.ligru_use_batchnorm = [True]\n",
    "        self.ligru_use_laynorm = [False]\n",
    "        self.ligru_orthinit = True\n",
    "        self.bidir = False\n",
    "        self.use_cuda = use_cuda\n",
    "        #self.mode = mode\n",
    "        #self.test_flag = (self.mode != 'train')\n",
    "        \n",
    "        # List initialization\n",
    "        self.wh = nn.ModuleList([])\n",
    "        self.uh = nn.ModuleList([])\n",
    "        self.wz = nn.ModuleList([])\n",
    "        self.uz = nn.ModuleList([])\n",
    "\n",
    "        self.ln = nn.ModuleList([]) # Layer Norm\n",
    "        self.bn_wh = nn.ModuleList([]) # Batch Norm\n",
    "        self.bn_wz = nn.ModuleList([]) # Batch Norm\n",
    "        if self.ligru_use_laynorm_inp: self.ln0 = LayerNorm(self.input_dim) # Input layer normalization\n",
    "        if self.ligru_use_batchnorm_inp: self.bn0 = nn.BatchNorm1d(self.input_dim, momentum = 0.05) # Input batch normalization\n",
    "        \n",
    "        self.N_ligru_lay = len(self.ligru_lay)\n",
    "        current_input = self.input_dim\n",
    "        \n",
    "        # Initialization of hidden layers\n",
    "        for i in range(self.N_ligru_lay):\n",
    "            add_bias = True\n",
    "            if self.ligru_use_laynorm[i] or self.ligru_use_batchnorm[i]: add_bias = False\n",
    "            \n",
    "            # Feed-forward connections\n",
    "            self.wh.append(nn.Linear(current_input, self.ligru_lay[i], bias = add_bias))\n",
    "            self.wz.append(nn.Linear(current_input, self.ligru_lay[i], bias = add_bias))\n",
    "            # Recurrent connections\n",
    "            self.uh.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i], bias = False))\n",
    "            self.uz.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i], bias = False))\n",
    "\n",
    "            if self.ligru_orthinit:\n",
    "                nn.init.orthogonal_(self.uh[i].weight)\n",
    "                nn.init.orthogonal_(self.uz[i].weight)\n",
    "            \n",
    "            # batch norm initialization\n",
    "            self.bn_wh.append(nn.BatchNorm1d(self.ligru_lay[i], momentum = 0.05))\n",
    "            self.bn_wz.append(nn.BatchNorm1d(self.ligru_lay[i], momentum = 0.05))\n",
    "            self.ln.append(LayerNorm(self.ligru_lay[i]))\n",
    "            \n",
    "            if self.bidir: current_input = 2 * self.ligru_lay[i]\n",
    "            else: current_input = self.ligru_lay[i]\n",
    "        self.out_dim = self.ligru_lay[i] + self.bidir * self.ligru_lay[i]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Applying Layer/Batch Norm\n",
    "        if bool(self.ligru_use_laynorm_inp): x = self.ln0((x))\n",
    "        if bool(self.ligru_use_batchnorm_inp):\n",
    "            x_bn = self.bn0(x.view(x.shape[0] * x.shape[1], x.shape[2]))\n",
    "            x = x_bn.view(x.shape[0], x.shape[1], x.shape[2])\n",
    "        \n",
    "        for i in range(self.N_ligru_lay):\n",
    "            # Initial state and concatenation\n",
    "            if self.bidir:\n",
    "                h_init = torch.zeros(2*x.shape[1], self.ligru_lay[i])\n",
    "                x = torch.cat([x,flip(x,0)], 1)\n",
    "            else:\n",
    "                h_init = torch.zeros(x.shape[1], self.ligru_lay[i])\n",
    "            \n",
    "            # Drop mask initilization (same mask for all time steps)\n",
    "            in_size = [1]+list(x.size())[1:]\n",
    "            h_size = list(h_init.size()[:2])\n",
    "            if self.drop['in'][0] == 'stochastic':\n",
    "                sigma = self.drop['in'][1]\n",
    "                drop_mask_ih = torch.from_numpy(np.exp(np.random.normal(loc=-sigma**2/2, scale=sigma, size=in_size)).astype('f')) if self.training else torch.FloatTensor([1])\n",
    "                drop_mask_iz = torch.from_numpy(np.exp(np.random.normal(loc=-sigma**2/2, scale=sigma, size=in_size)).astype('f')) if self.training else torch.FloatTensor([1])\n",
    "            else:\n",
    "                prob = self.drop['in'][1]\n",
    "                drop_mask_ih = torch.bernoulli(torch.Tensor(*in_size).fill_(1-prob)) if self.training else torch.FloatTensor([1-prob])\n",
    "                drop_mask_iz = torch.bernoulli(torch.Tensor(*in_size).fill_(1-prob)) if self.training else torch.FloatTensor([1-prob])\n",
    "            if self.drop['recur'][0] == 'stochastic':\n",
    "                sigma = self.drop['recur'][1]\n",
    "                drop_mask_rh = torch.from_numpy(np.exp(np.random.normal(loc=-sigma**2/2, scale=sigma, size=h_size)).astype('f')) if self.training else torch.FloatTensor([1])\n",
    "                drop_mask_rz = torch.from_numpy(np.exp(np.random.normal(loc=-sigma**2/2, scale=sigma, size=h_size)).astype('f')) if self.training else torch.FloatTensor([1])\n",
    "            else:\n",
    "                prob = self.drop['recur'][1]\n",
    "                drop_mask_rh = torch.bernoulli(torch.Tensor(*h_size).fill_(1-prob)) if self.training else torch.FloatTensor([1-prob])\n",
    "                drop_mask_rz = torch.bernoulli(torch.Tensor(*h_size).fill_(1-prob)) if self.training else torch.FloatTensor([1-prob])\n",
    "            if self.drop['orig'][0] == 'stochastic':\n",
    "                sigma = self.drop['orig'][1]\n",
    "                drop_mask_o = torch.from_numpy(np.exp(np.random.normal(loc=-sigma**2/2, scale=sigma, size=h_size)).astype('f')) if self.training else torch.FloatTensor([1])\n",
    "            else:\n",
    "                prob = self.drop['orig'][1]\n",
    "                drop_mask_o = torch.bernoulli(torch.Tensor(*h_size).fill_(1-prob)) if self.training else torch.FloatTensor([1-prob])\n",
    "                \n",
    "            if self.use_cuda:\n",
    "                h_init = h_init.cuda()\n",
    "                drop_mask_ih = drop_mask_ih.cuda()\n",
    "                drop_mask_iz = drop_mask_ih.cuda()\n",
    "                drop_mask_rh = drop_mask_rh.cuda()\n",
    "                drop_mask_rz = drop_mask_rz.cuda()\n",
    "                drop_mask_o = drop_mask_o.cuda()\n",
    "            \n",
    "            # Feed-forward affine transformations (all steps in parallel)\n",
    "            wh_out = self.wh[i](x * drop_mask_ih)\n",
    "            wz_out = self.wz[i](x * drop_mask_iz)\n",
    "\n",
    "            # Apply batch norm if needed (all steos in parallel)\n",
    "            if self.ligru_use_batchnorm[i]:\n",
    "                wh_out_bn = self.bn_wh[i](wh_out.view(wh_out.shape[0] * wh_out.shape[1], wh_out.shape[2]))\n",
    "                wh_out = wh_out_bn.view(wh_out.shape[0], wh_out.shape[1], wh_out.shape[2])\n",
    "                wz_out_bn = self.bn_wz[i](wz_out.view(wz_out.shape[0] * wz_out.shape[1], wz_out.shape[2]))\n",
    "                wz_out = wz_out_bn.view(wz_out.shape[0], wz_out.shape[1], wz_out.shape[2])\n",
    "\n",
    "            # Processing time steps\n",
    "            hiddens = []\n",
    "            ht = h_init\n",
    "            for k in range(x.shape[0]):\n",
    "                # ligru equation\n",
    "                zt = torch.sigmoid(wz_out[k] + self.uz[i](ht * drop_mask_rz))\n",
    "                #zt = HardSigmoid()(wz_out[k] + self.uz[i](ht * rdrop_mask1))\n",
    "                at = wh_out[k] + self.uh[i](ht * drop_mask_rh)\n",
    "                hcand = F.relu(at) * drop_mask_o\n",
    "                ht = (zt * ht + (1-zt) * hcand)\n",
    "                \n",
    "                if self.ligru_use_laynorm[i]: ht = self.ln[i](ht)\n",
    "                hiddens.append(ht)\n",
    "            h = torch.stack(hiddens)\n",
    "            \n",
    "            # Bidirectional concatenations\n",
    "            if self.bidir:\n",
    "                h_f = h[:,0:int(x.shape[1]/2)]\n",
    "                h_b = flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n",
    "                h = torch.cat([h_f,h_b],2)\n",
    "            \n",
    "            # Setup x for the next hidden layer\n",
    "            x = h\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_shape):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        self.oc = 128\n",
    "        #self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.oc, kernel_size=(3,3), stride=(1,1), padding=(1,1), bias=True)\n",
    "        self.res1 = ResLayer(in_channels= 1, out_channels=32, momentum=0.05, maxpool=True)\n",
    "        self.res2 = ResLayer(in_channels=32, out_channels=64, momentum=0.05, maxpool=True)\n",
    "        self.res3 = ResLayer(in_channels=64, out_channels=self.oc, momentum=0.05, maxpool=False)\n",
    "        self.recur1 = liGRU(256, self.oc * self.in_shape[0]//4, use_cuda=True)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.sdo = StochasticDropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        #x = F.relu(self.conv1(x))\n",
    "        x = self.res3(self.res2(self.res1(x)))\n",
    "        x = x.view(-1, self.oc * self.in_shape[0]//4, self.in_shape[1]//4).permute(2, 0, 1).contiguous()\n",
    "        x = self.recur1(x)[-1]\n",
    "        #x = x.permute(1,0,2).contiguous().view(-1,96*512) # for bidirectional\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.sdo(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danvilla/anaconda2/envs/py37/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/danvilla/anaconda2/envs/py37/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/100 - Step 087/087 (100.0%) - LR 9.80e-04 - TrLoss 1.7525 - TrAcc 0.3776 - VaLoss 0.9386 - VaAcc 0.6758 - Time 19.2s *\n",
      "Epoch 002/100 - Step 087/087 (100.0%) - LR 9.60e-04 - TrLoss 0.6167 - TrAcc 0.7882 - VaLoss 0.3887 - VaAcc 0.8759 - Time 18.7s *\n",
      "Epoch 003/100 - Step 087/087 (100.0%) - LR 9.41e-04 - TrLoss 0.3648 - TrAcc 0.8842 - VaLoss 0.2604 - VaAcc 0.9159 - Time 18.0s *\n",
      "Epoch 004/100 - Step 087/087 (100.0%) - LR 9.22e-04 - TrLoss 0.2621 - TrAcc 0.9155 - VaLoss 0.2598 - VaAcc 0.9192 - Time 17.8s *\n",
      "Epoch 005/100 - Step 087/087 (100.0%) - LR 9.04e-04 - TrLoss 0.2143 - TrAcc 0.9333 - VaLoss 0.1961 - VaAcc 0.9387 - Time 18.8s *\n",
      "Epoch 006/100 - Step 087/087 (100.0%) - LR 8.86e-04 - TrLoss 0.1945 - TrAcc 0.9393 - VaLoss 0.1916 - VaAcc 0.9374 - Time 18.1s \n",
      "Epoch 007/100 - Step 087/087 (100.0%) - LR 8.68e-04 - TrLoss 0.1701 - TrAcc 0.9463 - VaLoss 0.1591 - VaAcc 0.9524 - Time 18.9s *\n",
      "Epoch 008/100 - Step 087/087 (100.0%) - LR 8.51e-04 - TrLoss 0.1617 - TrAcc 0.9491 - VaLoss 0.1779 - VaAcc 0.9495 - Time 18.6s \n",
      "Epoch 009/100 - Step 087/087 (100.0%) - LR 8.34e-04 - TrLoss 0.1463 - TrAcc 0.9527 - VaLoss 0.1586 - VaAcc 0.9514 - Time 18.5s \n",
      "Epoch 010/100 - Step 087/087 (100.0%) - LR 8.17e-04 - TrLoss 0.1326 - TrAcc 0.9581 - VaLoss 0.1439 - VaAcc 0.9560 - Time 18.8s *\n",
      "Epoch 011/100 - Step 087/087 (100.0%) - LR 8.01e-04 - TrLoss 0.1234 - TrAcc 0.9621 - VaLoss 0.1472 - VaAcc 0.9550 - Time 18.8s \n",
      "Epoch 012/100 - Step 087/087 (100.0%) - LR 7.85e-04 - TrLoss 0.1201 - TrAcc 0.9626 - VaLoss 0.1481 - VaAcc 0.9544 - Time 18.5s \n",
      "Epoch 013/100 - Step 087/087 (100.0%) - LR 7.69e-04 - TrLoss 0.1093 - TrAcc 0.9643 - VaLoss 0.1890 - VaAcc 0.9436 - Time 19.1s \n",
      "Epoch 014/100 - Step 087/087 (100.0%) - LR 7.54e-04 - TrLoss 0.1017 - TrAcc 0.9678 - VaLoss 0.1737 - VaAcc 0.9495 - Time 18.5s \n",
      "Epoch 015/100 - Step 087/087 (100.0%) - LR 7.39e-04 - TrLoss 0.0999 - TrAcc 0.9683 - VaLoss 0.1354 - VaAcc 0.9560 - Time 19.0s \n",
      "Epoch 016/100 - Step 087/087 (100.0%) - LR 7.24e-04 - TrLoss 0.0988 - TrAcc 0.9679 - VaLoss 0.1348 - VaAcc 0.9593 - Time 18.7s *\n",
      "Epoch 017/100 - Step 087/087 (100.0%) - LR 7.09e-04 - TrLoss 0.0969 - TrAcc 0.9687 - VaLoss 0.1357 - VaAcc 0.9570 - Time 18.6s \n",
      "Epoch 018/100 - Step 087/087 (100.0%) - LR 6.95e-04 - TrLoss 0.0981 - TrAcc 0.9683 - VaLoss 0.1331 - VaAcc 0.9606 - Time 18.6s *\n",
      "Epoch 019/100 - Step 087/087 (100.0%) - LR 6.81e-04 - TrLoss 0.0890 - TrAcc 0.9712 - VaLoss 0.1346 - VaAcc 0.9609 - Time 18.4s *\n",
      "Epoch 020/100 - Step 087/087 (100.0%) - LR 6.68e-04 - TrLoss 0.0879 - TrAcc 0.9711 - VaLoss 0.1396 - VaAcc 0.9602 - Time 17.9s \n",
      "Epoch 021/100 - Step 087/087 (100.0%) - LR 6.54e-04 - TrLoss 0.0799 - TrAcc 0.9730 - VaLoss 0.1381 - VaAcc 0.9619 - Time 18.7s *\n",
      "Epoch 022/100 - Step 087/087 (100.0%) - LR 6.41e-04 - TrLoss 0.0761 - TrAcc 0.9757 - VaLoss 0.1352 - VaAcc 0.9593 - Time 19.0s \n",
      "Epoch 023/100 - Step 087/087 (100.0%) - LR 6.28e-04 - TrLoss 0.0797 - TrAcc 0.9742 - VaLoss 0.1304 - VaAcc 0.9622 - Time 18.5s *\n",
      "Epoch 024/100 - Step 087/087 (100.0%) - LR 6.16e-04 - TrLoss 0.0626 - TrAcc 0.9798 - VaLoss 0.1482 - VaAcc 0.9612 - Time 18.7s \n",
      "Epoch 025/100 - Step 087/087 (100.0%) - LR 6.03e-04 - TrLoss 0.0749 - TrAcc 0.9754 - VaLoss 0.1607 - VaAcc 0.9547 - Time 18.3s \n",
      "Epoch 026/100 - Step 087/087 (100.0%) - LR 5.91e-04 - TrLoss 0.0694 - TrAcc 0.9771 - VaLoss 0.1547 - VaAcc 0.9593 - Time 18.4s \n",
      "Epoch 027/100 - Step 087/087 (100.0%) - LR 5.80e-04 - TrLoss 0.0687 - TrAcc 0.9778 - VaLoss 0.1524 - VaAcc 0.9586 - Time 18.9s \n",
      "Epoch 028/100 - Step 087/087 (100.0%) - LR 5.68e-04 - TrLoss 0.0638 - TrAcc 0.9792 - VaLoss 0.1506 - VaAcc 0.9570 - Time 18.4s \n",
      "Epoch 029/100 - Step 087/087 (100.0%) - LR 5.57e-04 - TrLoss 0.0666 - TrAcc 0.9779 - VaLoss 0.1441 - VaAcc 0.9606 - Time 18.4s \n",
      "Epoch 030/100 - Step 087/087 (100.0%) - LR 5.45e-04 - TrLoss 0.0608 - TrAcc 0.9806 - VaLoss 0.1839 - VaAcc 0.9547 - Time 17.5s \n",
      "Epoch 031/100 - Step 087/087 (100.0%) - LR 5.35e-04 - TrLoss 0.0589 - TrAcc 0.9817 - VaLoss 0.1403 - VaAcc 0.9642 - Time 17.4s *\n",
      "Epoch 032/100 - Step 087/087 (100.0%) - LR 5.24e-04 - TrLoss 0.0552 - TrAcc 0.9823 - VaLoss 0.1586 - VaAcc 0.9573 - Time 19.0s \n",
      "Epoch 033/100 - Step 087/087 (100.0%) - LR 5.13e-04 - TrLoss 0.0560 - TrAcc 0.9824 - VaLoss 0.1216 - VaAcc 0.9638 - Time 18.3s \n",
      "Epoch 034/100 - Step 087/087 (100.0%) - LR 5.03e-04 - TrLoss 0.0617 - TrAcc 0.9804 - VaLoss 0.1396 - VaAcc 0.9606 - Time 18.7s \n",
      "Epoch 035/100 - Step 087/087 (100.0%) - LR 4.93e-04 - TrLoss 0.0509 - TrAcc 0.9834 - VaLoss 0.1528 - VaAcc 0.9586 - Time 18.5s \n",
      "Epoch 036/100 - Step 087/087 (100.0%) - LR 4.83e-04 - TrLoss 0.0573 - TrAcc 0.9816 - VaLoss 0.1390 - VaAcc 0.9632 - Time 19.1s \n",
      "Epoch 037/100 - Step 087/087 (100.0%) - LR 4.74e-04 - TrLoss 0.0541 - TrAcc 0.9831 - VaLoss 0.1528 - VaAcc 0.9602 - Time 19.0s \n",
      "Epoch 038/100 - Step 087/087 (100.0%) - LR 4.64e-04 - TrLoss 0.0519 - TrAcc 0.9833 - VaLoss 0.1528 - VaAcc 0.9602 - Time 18.7s \n",
      "Epoch 039/100 - Step 087/087 (100.0%) - LR 4.55e-04 - TrLoss 0.0528 - TrAcc 0.9836 - VaLoss 0.1343 - VaAcc 0.9625 - Time 18.6s \n",
      "Epoch 040/100 - Step 087/087 (100.0%) - LR 4.46e-04 - TrLoss 0.0433 - TrAcc 0.9865 - VaLoss 0.1370 - VaAcc 0.9638 - Time 18.8s \n",
      "Epoch 041/100 - Step 087/087 (100.0%) - LR 4.37e-04 - TrLoss 0.0456 - TrAcc 0.9857 - VaLoss 0.1221 - VaAcc 0.9616 - Time 18.9s \n",
      "Epoch 042/100 - Step 087/087 (100.0%) - LR 4.28e-04 - TrLoss 0.0479 - TrAcc 0.9849 - VaLoss 0.1311 - VaAcc 0.9622 - Time 18.6s \n",
      "Epoch 043/100 - Step 087/087 (100.0%) - LR 4.19e-04 - TrLoss 0.0452 - TrAcc 0.9857 - VaLoss 0.1305 - VaAcc 0.9602 - Time 18.8s \n",
      "Epoch 044/100 - Step 087/087 (100.0%) - LR 4.11e-04 - TrLoss 0.0374 - TrAcc 0.9880 - VaLoss 0.1317 - VaAcc 0.9625 - Time 18.8s \n",
      "Epoch 045/100 - Step 087/087 (100.0%) - LR 4.03e-04 - TrLoss 0.0426 - TrAcc 0.9870 - VaLoss 0.1395 - VaAcc 0.9593 - Time 17.9s \n",
      "Epoch 046/100 - Step 087/087 (100.0%) - LR 3.95e-04 - TrLoss 0.0414 - TrAcc 0.9864 - VaLoss 0.1450 - VaAcc 0.9616 - Time 18.7s \n",
      "Epoch 047/100 - Step 087/087 (100.0%) - LR 3.87e-04 - TrLoss 0.0419 - TrAcc 0.9876 - VaLoss 0.1309 - VaAcc 0.9635 - Time 18.4s \n",
      "Epoch 048/100 - Step 087/087 (100.0%) - LR 3.79e-04 - TrLoss 0.0447 - TrAcc 0.9860 - VaLoss 0.1340 - VaAcc 0.9629 - Time 18.4s \n",
      "Epoch 049/100 - Step 087/087 (100.0%) - LR 3.72e-04 - TrLoss 0.0396 - TrAcc 0.9876 - VaLoss 0.1498 - VaAcc 0.9625 - Time 18.6s \n",
      "Epoch 050/100 - Step 087/087 (100.0%) - LR 3.64e-04 - TrLoss 0.0380 - TrAcc 0.9881 - VaLoss 0.1511 - VaAcc 0.9632 - Time 18.2s \n",
      "Epoch 051/100 - Step 087/087 (100.0%) - LR 3.57e-04 - TrLoss 0.0347 - TrAcc 0.9885 - VaLoss 0.1528 - VaAcc 0.9606 - Time 18.8s \n",
      "Epoch 052/100 - Step 087/087 (100.0%) - LR 3.50e-04 - TrLoss 0.0377 - TrAcc 0.9883 - VaLoss 0.1367 - VaAcc 0.9635 - Time 18.4s \n",
      "Epoch 053/100 - Step 087/087 (100.0%) - LR 3.43e-04 - TrLoss 0.0333 - TrAcc 0.9899 - VaLoss 0.1415 - VaAcc 0.9629 - Time 18.0s \n",
      "Epoch 054/100 - Step 087/087 (100.0%) - LR 3.36e-04 - TrLoss 0.0330 - TrAcc 0.9892 - VaLoss 0.1495 - VaAcc 0.9629 - Time 18.2s \n",
      "Epoch 055/100 - Step 087/087 (100.0%) - LR 3.29e-04 - TrLoss 0.0355 - TrAcc 0.9889 - VaLoss 0.1496 - VaAcc 0.9593 - Time 19.0s \n",
      "Epoch 056/100 - Step 087/087 (100.0%) - LR 3.23e-04 - TrLoss 0.0366 - TrAcc 0.9882 - VaLoss 0.1431 - VaAcc 0.9635 - Time 19.1s \n",
      "Epoch 057/100 - Step 087/087 (100.0%) - LR 3.16e-04 - TrLoss 0.0281 - TrAcc 0.9909 - VaLoss 0.1696 - VaAcc 0.9642 - Time 17.5s \n",
      "Epoch 058/100 - Step 087/087 (100.0%) - LR 3.10e-04 - TrLoss 0.0297 - TrAcc 0.9903 - VaLoss 0.1650 - VaAcc 0.9616 - Time 18.8s \n",
      "Epoch 059/100 - Step 087/087 (100.0%) - LR 3.04e-04 - TrLoss 0.0278 - TrAcc 0.9905 - VaLoss 0.1545 - VaAcc 0.9629 - Time 18.9s \n",
      "Epoch 060/100 - Step 087/087 (100.0%) - LR 2.98e-04 - TrLoss 0.0332 - TrAcc 0.9891 - VaLoss 0.1523 - VaAcc 0.9596 - Time 19.0s \n",
      "Epoch 061/100 - Step 087/087 (100.0%) - LR 2.92e-04 - TrLoss 0.0316 - TrAcc 0.9897 - VaLoss 0.1553 - VaAcc 0.9632 - Time 18.3s \n",
      "Epoch 062/100 - Step 087/087 (100.0%) - LR 2.86e-04 - TrLoss 0.0290 - TrAcc 0.9904 - VaLoss 0.1534 - VaAcc 0.9638 - Time 17.5s \n",
      "Epoch 063/100 - Step 087/087 (100.0%) - LR 2.80e-04 - TrLoss 0.0294 - TrAcc 0.9904 - VaLoss 0.1601 - VaAcc 0.9583 - Time 18.3s \n",
      "Epoch 064/100 - Step 087/087 (100.0%) - LR 2.74e-04 - TrLoss 0.0265 - TrAcc 0.9916 - VaLoss 0.1459 - VaAcc 0.9635 - Time 17.8s \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 065/100 - Step 087/087 (100.0%) - LR 2.69e-04 - TrLoss 0.0221 - TrAcc 0.9926 - VaLoss 0.1530 - VaAcc 0.9638 - Time 17.6s \n",
      "Epoch 066/100 - Step 087/087 (100.0%) - LR 2.64e-04 - TrLoss 0.0242 - TrAcc 0.9920 - VaLoss 0.1586 - VaAcc 0.9635 - Time 17.9s \n",
      "Epoch 067/100 - Step 087/087 (100.0%) - LR 2.58e-04 - TrLoss 0.0307 - TrAcc 0.9905 - VaLoss 0.1435 - VaAcc 0.9642 - Time 18.8s \n",
      "Epoch 068/100 - Step 087/087 (100.0%) - LR 2.53e-04 - TrLoss 0.0238 - TrAcc 0.9931 - VaLoss 0.1789 - VaAcc 0.9602 - Time 18.6s \n",
      "Epoch 069/100 - Step 087/087 (100.0%) - LR 2.48e-04 - TrLoss 0.0214 - TrAcc 0.9932 - VaLoss 0.1619 - VaAcc 0.9606 - Time 18.7s \n",
      "Epoch 070/100 - Step 087/087 (100.0%) - LR 2.43e-04 - TrLoss 0.0237 - TrAcc 0.9925 - VaLoss 0.1569 - VaAcc 0.9609 - Time 18.6s \n",
      "Epoch 071/100 - Step 087/087 (100.0%) - LR 2.38e-04 - TrLoss 0.0249 - TrAcc 0.9925 - VaLoss 0.1641 - VaAcc 0.9616 - Time 18.7s \n",
      "Epoch 072/100 - Step 087/087 (100.0%) - LR 2.33e-04 - TrLoss 0.0267 - TrAcc 0.9912 - VaLoss 0.1420 - VaAcc 0.9648 - Time 18.7s *\n",
      "Epoch 073/100 - Step 087/087 (100.0%) - LR 2.29e-04 - TrLoss 0.0228 - TrAcc 0.9926 - VaLoss 0.1404 - VaAcc 0.9648 - Time 18.1s \n",
      "Epoch 074/100 - Step 087/087 (100.0%) - LR 2.24e-04 - TrLoss 0.0206 - TrAcc 0.9931 - VaLoss 0.1521 - VaAcc 0.9655 - Time 18.8s *\n",
      "Epoch 075/100 - Step 087/087 (100.0%) - LR 2.20e-04 - TrLoss 0.0196 - TrAcc 0.9935 - VaLoss 0.1570 - VaAcc 0.9638 - Time 18.0s \n",
      "Epoch 076/100 - Step 087/087 (100.0%) - LR 2.15e-04 - TrLoss 0.0203 - TrAcc 0.9936 - VaLoss 0.1559 - VaAcc 0.9668 - Time 18.4s *\n",
      "Epoch 077/100 - Step 087/087 (100.0%) - LR 2.11e-04 - TrLoss 0.0210 - TrAcc 0.9930 - VaLoss 0.1776 - VaAcc 0.9616 - Time 18.3s \n",
      "Epoch 078/100 - Step 087/087 (100.0%) - LR 2.07e-04 - TrLoss 0.0195 - TrAcc 0.9938 - VaLoss 0.1562 - VaAcc 0.9661 - Time 18.9s \n",
      "Epoch 079/100 - Step 087/087 (100.0%) - LR 2.03e-04 - TrLoss 0.0218 - TrAcc 0.9935 - VaLoss 0.1411 - VaAcc 0.9671 - Time 18.9s *\n",
      "Epoch 080/100 - Step 087/087 (100.0%) - LR 1.99e-04 - TrLoss 0.0245 - TrAcc 0.9919 - VaLoss 0.1479 - VaAcc 0.9622 - Time 19.0s \n",
      "Epoch 081/100 - Step 087/087 (100.0%) - LR 1.95e-04 - TrLoss 0.0173 - TrAcc 0.9945 - VaLoss 0.1564 - VaAcc 0.9642 - Time 19.3s \n",
      "Epoch 082/100 - Step 087/087 (100.0%) - LR 1.91e-04 - TrLoss 0.0197 - TrAcc 0.9940 - VaLoss 0.1431 - VaAcc 0.9645 - Time 19.0s \n",
      "Epoch 083/100 - Step 087/087 (100.0%) - LR 1.87e-04 - TrLoss 0.0202 - TrAcc 0.9938 - VaLoss 0.1629 - VaAcc 0.9655 - Time 19.2s \n",
      "Epoch 084/100 - Step 087/087 (100.0%) - LR 1.83e-04 - TrLoss 0.0231 - TrAcc 0.9923 - VaLoss 0.1394 - VaAcc 0.9661 - Time 18.5s \n",
      "Epoch 085/100 - Step 087/087 (100.0%) - LR 1.80e-04 - TrLoss 0.0145 - TrAcc 0.9949 - VaLoss 0.1466 - VaAcc 0.9651 - Time 18.9s \n",
      "Epoch 086/100 - Step 087/087 (100.0%) - LR 1.76e-04 - TrLoss 0.0182 - TrAcc 0.9944 - VaLoss 0.1659 - VaAcc 0.9629 - Time 18.6s \n",
      "Epoch 087/100 - Step 087/087 (100.0%) - LR 1.72e-04 - TrLoss 0.0165 - TrAcc 0.9945 - VaLoss 0.1711 - VaAcc 0.9602 - Time 18.6s \n",
      "Epoch 088/100 - Step 087/087 (100.0%) - LR 1.69e-04 - TrLoss 0.0193 - TrAcc 0.9939 - VaLoss 0.1781 - VaAcc 0.9619 - Time 18.8s \n",
      "Epoch 089/100 - Step 087/087 (100.0%) - LR 1.66e-04 - TrLoss 0.0202 - TrAcc 0.9934 - VaLoss 0.1600 - VaAcc 0.9625 - Time 18.6s \n",
      "Epoch 090/100 - Step 087/087 (100.0%) - LR 1.62e-04 - TrLoss 0.0213 - TrAcc 0.9934 - VaLoss 0.1635 - VaAcc 0.9609 - Time 18.7s \n",
      "Epoch 091/100 - Step 087/087 (100.0%) - LR 1.59e-04 - TrLoss 0.0192 - TrAcc 0.9944 - VaLoss 0.1501 - VaAcc 0.9625 - Time 18.5s \n",
      "Epoch 092/100 - Step 087/087 (100.0%) - LR 1.56e-04 - TrLoss 0.0167 - TrAcc 0.9949 - VaLoss 0.1515 - VaAcc 0.9622 - Time 18.2s \n",
      "Epoch 093/100 - Step 087/087 (100.0%) - LR 1.53e-04 - TrLoss 0.0167 - TrAcc 0.9950 - VaLoss 0.1598 - VaAcc 0.9638 - Time 18.7s \n",
      "Epoch 094/100 - Step 087/087 (100.0%) - LR 1.50e-04 - TrLoss 0.0178 - TrAcc 0.9942 - VaLoss 0.1430 - VaAcc 0.9664 - Time 19.1s \n",
      "Epoch 095/100 - Step 087/087 (100.0%) - LR 1.47e-04 - TrLoss 0.0154 - TrAcc 0.9947 - VaLoss 0.1494 - VaAcc 0.9651 - Time 18.3s \n",
      "Epoch 096/100 - Step 087/087 (100.0%) - LR 1.44e-04 - TrLoss 0.0149 - TrAcc 0.9953 - VaLoss 0.1427 - VaAcc 0.9661 - Time 18.7s \n",
      "Epoch 097/100 - Step 087/087 (100.0%) - LR 1.41e-04 - TrLoss 0.0188 - TrAcc 0.9940 - VaLoss 0.1447 - VaAcc 0.9661 - Time 18.8s \n",
      "Epoch 098/100 - Step 063/087 ( 72.4%) - LR 1.38e-04 - TrLoss 0.0151 - TrAcc 0.9952"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 256\n",
    "iters  = (Xtr.shape[0] + batch_size - 1) // batch_size\n",
    "device = torch.device('cuda')\n",
    "\n",
    "v = 12\n",
    "model_F_file = 'models/F%03d.pt'%v\n",
    "\n",
    "lr = 1e-3\n",
    "model = Net(Xtr.shape[1:]).to(device)\n",
    "obj = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lambda epoch: 0.98 ** epoch)\n",
    "\n",
    "def run_eval(Xev, yev):\n",
    "    yp = []\n",
    "    cur_loss = 0\n",
    "    cur_acc  = 0\n",
    "    model.eval()\n",
    "    for step in range((Xev.shape[0] + batch_size - 1) // batch_size):\n",
    "        x = torch.from_numpy(Xev[step * batch_size : (step + 1) * batch_size]).float().to(device)\n",
    "        y = torch.from_numpy(yev[step * batch_size : (step + 1) * batch_size]).to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "        yp.append(y_pred.cpu().numpy())\n",
    "        cur_loss += obj(y_pred, y).item()\n",
    "        cur_acc  += np.sum((y_pred.max(dim=1)[1] == y).cpu().numpy())\n",
    "    return np.argmax(np.vstack(yp), -1), cur_loss / Xev.shape[0], cur_acc / Xev.shape[0]\n",
    "\n",
    "best_acc = 0\n",
    "hist = {'TrLoss':[], 'TrAcc':[], 'VaLoss':[], 'VaAcc':[]}\n",
    "for epoch in range(epochs):\n",
    "    scheduler.step()\n",
    "    cur_samp = 0\n",
    "    cur_loss = 0\n",
    "    cur_acc  = 0\n",
    "    t0 = time.time()\n",
    "    for step in range(iters):\n",
    "        use = np.random.choice(range(Xtr.shape[0]), batch_size, replace=False)\n",
    "        noise = np.random.uniform(0, 0.3) * np.random.randn(batch_size, *Xtr.shape[1:])\n",
    "        noise += np.random.uniform(0, 0.3) * np.random.randn(batch_size, Xtr.shape[1], 1)\n",
    "        x = torch.from_numpy(Xtr[use] + noise).float().to(device)\n",
    "        y = torch.from_numpy(ytr[use]).to(device)\n",
    "        \n",
    "        model.train()\n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = obj(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        cur_samp += batch_size\n",
    "        cur_loss += loss.item()\n",
    "        cur_acc  += np.sum((y_pred.max(dim=1)[1] == y).cpu().numpy())\n",
    "        lr = scheduler.get_lr()[-1]\n",
    "        print('\\rEpoch %03d/%03d - Step %03d/%03d (%5.1f%%) - LR %.2e - TrLoss %.4f - TrAcc %.4f'%(\n",
    "            epoch+1, epochs, step+1, iters, 100*(step+1)/iters, lr, cur_loss / cur_samp, cur_acc / cur_samp), end='')\n",
    "    \n",
    "    _, loss, acc = run_eval(Xva, yva)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        cur_best = True\n",
    "        torch.save(model.state_dict(), model_F_file)\n",
    "    else:\n",
    "        cur_best = False\n",
    "    print(' - VaLoss %.4f - VaAcc %.4f - Time %4.1fs %s'%(loss, acc, time.time() - t0, '*' if cur_best else ''))\n",
    "    hist['TrLoss'].append(cur_loss / cur_samp)\n",
    "    hist['TrAcc' ].append(cur_acc / cur_samp)\n",
    "    hist['VaLoss'].append(loss)\n",
    "    hist['VaAcc' ].append(acc)\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(model_F_file))\n",
    "pte, loss, acc = run_eval(Xte, yte)\n",
    "print('TeLoss %.4f - TeAcc %.4f'%(loss, acc))\n",
    "\n",
    "print()\n",
    "print('Data version ---- %s'%data_version)\n",
    "print('Saved in -------- %s'%model_F_file)\n",
    "print('Results --------- F%.2f%% - T%.2f%%'%(100*best_acc, 100*acc))\n",
    "metrics(pte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder + data_version + '_submit.pkl', 'rb') as f:\n",
    "    fnames = pickle.load(f)\n",
    "with open(data_folder + data_version + '_Xsub.npy', 'rb') as f:\n",
    "    Xsub = np.load(f)\n",
    "Xsub = (Xsub - mu) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psub = model.predict(Xsub)\n",
    "preds = np.argmax(psub, -1)\n",
    "pred_names = map(lambda x: classes[x], preds)\n",
    "with open('submission-05.txt', 'w') as f:\n",
    "    f.write('fname,label\\n')\n",
    "    for i in range(len(fnames)):\n",
    "        f.write('%s,%s\\n'%(fnames[i], pred_names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5b120879956f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mliGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliGRU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class liGRU(nn.Module):\n",
    "    def __init__(self, units, inp_dim, use_cuda=False):\n",
    "        super(liGRU, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.input_dim = inp_dim\n",
    "        self.ligru_lay = [units]\n",
    "        self.ligru_drop = [0.1]\n",
    "        self.ligru_rdrop = [0.1]\n",
    "        self.sigma_do = 0.3\n",
    "        self.sigma_dr = 0.3\n",
    "        self.ligru_use_batchnorm_inp = False\n",
    "        self.ligru_use_laynorm_inp = False\n",
    "        self.ligru_use_batchnorm = [True]\n",
    "        self.ligru_use_laynorm = [False]\n",
    "        self.ligru_orthinit = True\n",
    "        self.bidir = False\n",
    "        self.use_cuda = use_cuda\n",
    "        #self.mode = mode\n",
    "        #self.test_flag = (self.mode != 'train')\n",
    "        \n",
    "        # List initialization\n",
    "        self.wh = nn.ModuleList([])\n",
    "        self.uh = nn.ModuleList([])\n",
    "        self.wz = nn.ModuleList([])\n",
    "        self.uz = nn.ModuleList([])\n",
    "\n",
    "        self.ln = nn.ModuleList([]) # Layer Norm\n",
    "        self.bn_wh = nn.ModuleList([]) # Batch Norm\n",
    "        self.bn_wz = nn.ModuleList([]) # Batch Norm\n",
    "        if self.ligru_use_laynorm_inp: self.ln0 = LayerNorm(self.input_dim) # Input layer normalization\n",
    "        if self.ligru_use_batchnorm_inp: self.bn0 = nn.BatchNorm1d(self.input_dim, momentum = 0.05) # Input batch normalization\n",
    "        \n",
    "        self.N_ligru_lay = len(self.ligru_lay)\n",
    "        current_input = self.input_dim\n",
    "        \n",
    "        # Initialization of hidden layers\n",
    "        for i in range(self.N_ligru_lay):\n",
    "            add_bias = True\n",
    "            if self.ligru_use_laynorm[i] or self.ligru_use_batchnorm[i]: add_bias = False\n",
    "            \n",
    "            # Feed-forward connections\n",
    "            self.wh.append(nn.Linear(current_input, self.ligru_lay[i], bias = add_bias))\n",
    "            self.wz.append(nn.Linear(current_input, self.ligru_lay[i], bias = add_bias))\n",
    "            # Recurrent connections\n",
    "            self.uh.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i], bias = False))\n",
    "            self.uz.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i], bias = False))\n",
    "\n",
    "            if self.ligru_orthinit:\n",
    "                nn.init.orthogonal_(self.uh[i].weight)\n",
    "                nn.init.orthogonal_(self.uz[i].weight)\n",
    "            \n",
    "            # batch norm initialization\n",
    "            self.bn_wh.append(nn.BatchNorm1d(self.ligru_lay[i], momentum = 0.05))\n",
    "            self.bn_wz.append(nn.BatchNorm1d(self.ligru_lay[i], momentum = 0.05))\n",
    "            self.ln.append(LayerNorm(self.ligru_lay[i]))\n",
    "            \n",
    "            if self.bidir: current_input = 2 * self.ligru_lay[i]\n",
    "            else: current_input = self.ligru_lay[i]\n",
    "        self.out_dim = self.ligru_lay[i] + self.bidir * self.ligru_lay[i]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Applying Layer/Batch Norm\n",
    "        if bool(self.ligru_use_laynorm_inp): x = self.ln0((x))\n",
    "        if bool(self.ligru_use_batchnorm_inp):\n",
    "            x_bn = self.bn0(x.view(x.shape[0] * x.shape[1], x.shape[2]))\n",
    "            x = x_bn.view(x.shape[0], x.shape[1], x.shape[2])\n",
    "        \n",
    "        for i in range(self.N_ligru_lay):\n",
    "            # Initial state and concatenation\n",
    "            if self.bidir:\n",
    "                h_init = torch.zeros(2*x.shape[1], self.ligru_lay[i])\n",
    "                x = torch.cat([x,flip(x,0)], 1)\n",
    "            else:\n",
    "                h_init = torch.zeros(x.shape[1], self.ligru_lay[i])\n",
    "            \n",
    "            # Drop mask initilization (same mask for all time steps)            \n",
    "            if self.training:\n",
    "                #drop_mask   = torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.ligru_drop[i]))\n",
    "                #rdrop_mask1 = torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.ligru_rdrop[i]))\n",
    "                #rdrop_mask2 = torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.ligru_rdrop[i]))\n",
    "                drop_mask = torch.from_numpy(np.exp(np.random.normal(loc=-self.sigma_do**2/2, scale=self.sigma_do, size=list(h_init.size()[:2]))).astype('f'))\n",
    "                rdrop_mask1 = torch.from_numpy(np.exp(np.random.normal(loc=-self.sigma_dr**2/2, scale=self.sigma_dr, size=list(h_init.size()[:2]))).astype('f'))\n",
    "                rdrop_mask2 = torch.from_numpy(np.exp(np.random.normal(loc=-self.sigma_dr**2/2, scale=self.sigma_dr, size=list(h_init.size()[:2]))).astype('f'))\n",
    "            else:\n",
    "                #drop_mask   = torch.FloatTensor([1-self.ligru_drop[i]])\n",
    "                #rdrop_mask1 = torch.FloatTensor([1-self.ligru_rdrop[i]])\n",
    "                #rdrop_mask2 = torch.FloatTensor([1-self.ligru_rdrop[i]])\n",
    "                drop_mask   = torch.FloatTensor([1])\n",
    "                rdrop_mask1 = torch.FloatTensor([1])\n",
    "                rdrop_mask2 = torch.FloatTensor([1])\n",
    "                \n",
    "            if self.use_cuda:\n",
    "                h_init = h_init.cuda()\n",
    "                drop_mask = drop_mask.cuda()\n",
    "                rdrop_mask1 = rdrop_mask1.cuda()\n",
    "                rdrop_mask2 = rdrop_mask2.cuda()\n",
    "            \n",
    "            # Feed-forward affine transformations (all steps in parallel)\n",
    "            wh_out = self.wh[i](x)\n",
    "            wz_out = self.wz[i](x)\n",
    "\n",
    "            # Apply batch norm if needed (all steos in parallel)\n",
    "            if self.ligru_use_batchnorm[i]:\n",
    "                wh_out_bn = self.bn_wh[i](wh_out.view(wh_out.shape[0] * wh_out.shape[1], wh_out.shape[2]))\n",
    "                wh_out = wh_out_bn.view(wh_out.shape[0], wh_out.shape[1], wh_out.shape[2])\n",
    "                wz_out_bn = self.bn_wz[i](wz_out.view(wz_out.shape[0] * wz_out.shape[1], wz_out.shape[2]))\n",
    "                wz_out = wz_out_bn.view(wz_out.shape[0], wz_out.shape[1], wz_out.shape[2])\n",
    "\n",
    "            # Processing time steps\n",
    "            hiddens = []\n",
    "            ht = h_init\n",
    "            for k in range(x.shape[0]):\n",
    "                # ligru equation\n",
    "                zt = torch.sigmoid(wz_out[k] + self.uz[i](ht * rdrop_mask1))\n",
    "                #zt = HardSigmoid()(wz_out[k] + self.uz[i](ht * rdrop_mask1))\n",
    "                at = wh_out[k] + self.uh[i](ht * rdrop_mask2)\n",
    "                hcand = F.relu(at) * drop_mask\n",
    "                ht = (zt * ht + (1-zt) * hcand)\n",
    "                \n",
    "                if self.ligru_use_laynorm[i]: ht = self.ln[i](ht)\n",
    "                hiddens.append(ht)\n",
    "            h = torch.stack(hiddens)\n",
    "            \n",
    "            # Bidirectional concatenations\n",
    "            if self.bidir:\n",
    "                h_f = h[:,0:int(x.shape[1]/2)]\n",
    "                h_b = flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n",
    "                h = torch.cat([h_f,h_b],2)\n",
    "            \n",
    "            # Setup x for the next hidden layer\n",
    "            x = h\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
